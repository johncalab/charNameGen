{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "We want a layer which takes in not just an input vector $x$ but a(n ordered) sequence of vectors $x_1,\\ldots,x_n$, where $x_i \\in R^d$.\n",
    "\n",
    "One thing we could do is stack them on top of one another, to create a big vector $\\vec x \\in R^{nd}$, and then apply a fully connected layer.\n",
    "But we want to be smarter than that.\n",
    "\n",
    "* An RNNcell is just a function $f(x,h)$ taking two inputs: the first is the input vector $x \\in R^d$, the second is called the *hidden* vector $h \\in R^k$.\n",
    "* The output of $y = f(x,h) \\in R^k$ has the same size as the hidden vector.\n",
    "* An RNN is built by stacking RNNcells.\n",
    "* Say the input is the sequence $(x_1,\\ldots,x_n)$ with $x_i \\in R^d$. We proceed step by step.\n",
    "    * $h_0 := \\vec 0$ (this can also be initialized differently)\n",
    "    * $h_1 := f(x_1,h_0)$\n",
    "    * $h_2 := f(x_2,h_1)$\n",
    "    * $h_3 := f(x_3,h_2)$\n",
    "    * ...\n",
    "    * $h_n := f(x_n,h_{n-1})$\n",
    "* The output of the RNN is the whole sequence $(h_1,\\ldots,h_n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see this in action using `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start by fixing n,k,d from above.\n",
    "(since it's pytorch, we must also work in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = seq_len = 15\n",
    "seq_len = 15\n",
    "# d = input_size = 7\n",
    "input_size = 7\n",
    "# k = hidden_size = 5\n",
    "hidden_size = 5\n",
    "\n",
    "# batch_len (aka batch_size) = 4\n",
    "batch_len = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now declare the RNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnlayer = nn.GRU(input_size=input_size,\n",
    "                 hidden_size=hidden_size,\n",
    "                 num_layers=1,\n",
    "                 bias=True,\n",
    "                 batch_first=True,\n",
    "                 dropout=0,\n",
    "                 bidirectional=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go over the parameters:\n",
    "\n",
    "- `input_size` and `hidden_size` we know about\n",
    "- `num_layers` says how many cells should be stacked on top of one another (the default is 1) [let's ignore this]\n",
    "- `bias` is the usual: adding a constant term (default = True).\n",
    "- `batch_first` decides if `input.shape` is (batch_size,seq_len,input_size) or (seq_len,batch_size,input_size). Same for outputs.\n",
    "    - `batch_first` is FALSE by default!\n",
    "- `dropout` is dropout, default = 0 [let's ignore this]\n",
    "- `bidirectional` is if we want a bidirectional RNN, default = False. [let's ignore this]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define some bogus inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bogus input\n",
    "batch_size = 4\n",
    "x = torch.randn(batch_size,seq_len,input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15, 7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "If we look closely, `rnnlayer` actually takes two inputs:\n",
    "\n",
    "- the first is the squence of input vectors, whose shape is (batch_len, seq_len, input_size).\n",
    "- the second is zero-th hidden vector, of shape: (1, batch_len, hidden_size).\n",
    "\n",
    "The reason for that funny 1 is that the real shape is (num_layers*num_directions, batch_len, hidden_size)\n",
    "- for us `num_layers = 1`, `num_directions = 1` (as bidirectional=False).\n",
    "\n",
    "By default, the second input is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is a tuple\n",
    "H, h_n = rnnlayer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `H` is the collection of all hidden vectors, with shape (batch_len, seq_len, hidden_size).\n",
    "\n",
    "(the actual shape is (batch_len, seq_len, num_directions*hidden_size), but num_directions=1 for us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15, 5])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `h_n` is the output of the last cell\n",
    "- `h_n` has shape (1,batch_len, hidden_size)\n",
    "\n",
    "The true shape is (num_layers*num_directions, batch_len, hidden_size), but num_layers*num_directions = 1 for us.\n",
    "- if you have more than layer, this allows you to separate the output from each, which may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 5])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
