{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNNs\n",
    "We want a layer which takes in not just an input vector $x$ but a(n ordered) sequence of vectors $x_1,\\ldots,x_n$, where $x_i \\in R^d$.\n",
    "\n",
    "One thing we could do is stack them on top of one another, to create a big vector $\\vec x \\in R^{nd}$, and then apply a fully connected layer.\n",
    "But we want to be smarter than that.\n",
    "\n",
    "* An RNNcell is just a function $f(x,h)$ taking two inputs: the first is the input vector $x \\in R^d$, the second is called the *hidden* vector $h \\in R^k$.\n",
    "* The output of $y = f(x,h) \\in R^k$ has the same size as the hidden vector.\n",
    "* An RNN is built by stacking RNNcells.\n",
    "* Say the input is the sequence $(x_1,\\ldots,x_n)$ with $x_i \\in R^d$. We proceed step by step.\n",
    "    * $h_0 := \\vec 0$ (this can also be initialized differently)\n",
    "    * $h_1 := f(x_1,h_0)$\n",
    "    * $h_2 := f(x_2,h_1)$\n",
    "    * $h_3 := f(x_3,h_2)$\n",
    "    * ...\n",
    "    * $h_n := f(x_n,h_{n-1})$\n",
    "* The output of the RNN is the whole sequence $(h_1,\\ldots,h_n)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see this in action using `pytorch`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We start by fixing n,k,d from above.\n",
    "(since it's pytorch, we must also work in batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n = seq_len = 15\n",
    "seq_len = 15\n",
    "# d = input_size = 7\n",
    "input_size = 7\n",
    "# k = hidden_size = 5\n",
    "hidden_size = 5\n",
    "# batch_len (aka batch_size) = 4\n",
    "batch_len = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now declare the RNN layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnlayer = nn.GRU(input_size=input_size,\n",
    "                 hidden_size=hidden_size,\n",
    "                 num_layers=1,\n",
    "                 bias=True,\n",
    "                 batch_first=True,\n",
    "                 dropout=0,\n",
    "                 bidirectional=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go over the parameters:\n",
    "\n",
    "- `input_size` and `hidden_size` we know about\n",
    "- `num_layers` says how many cells should be stacked on top of one another (the default is 1) [let's ignore this]\n",
    "- `bias` is the usual: adding a constant term (default = True).\n",
    "- `batch_first` decides if `input.shape` is (batch_size,seq_len,input_size) or (seq_len,batch_size,input_size). Same for outputs.\n",
    "    - `batch_first` is FALSE by default!\n",
    "- `dropout` is dropout, default = 0 [let's ignore this]\n",
    "- `bidirectional` is if we want a bidirectional RNN, default = False. [let's ignore this]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's define some bogus inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bogus input\n",
    "x = torch.randn(batch_len,seq_len,input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15, 7])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "If we look closely, `rnnlayer` actually takes two inputs:\n",
    "\n",
    "- the first is the squence of input vectors, whose shape is (batch_len, seq_len, input_size).\n",
    "- the second is zero-th hidden vector, of shape: (1, batch_len, hidden_size).\n",
    "\n",
    "The reason for that funny 1 is that the real shape is (num_layers*num_directions, batch_len, hidden_size)\n",
    "- for us `num_layers = 1`, `num_directions = 1` (as bidirectional=False).\n",
    "\n",
    "By default, the second input is 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's have a look at the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output is a tuple\n",
    "H, h_n = rnnlayer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `H` is the collection of all hidden vectors, with shape (batch_len, seq_len, hidden_size).\n",
    "\n",
    "(the actual shape is (batch_len, seq_len, num_directions*hidden_size), but num_directions=1 for us)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15, 5])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `h_n` is the output of the last cell\n",
    "- `h_n` has shape (1,batch_len, hidden_size)\n",
    "\n",
    "The true shape is (num_layers$*$num_directions, batch_len, hidden_size), but num_layers$*$num_directions = 1 for us.\n",
    "- if you have more than layer, this allows you to separate the output from each, which may be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 5])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_n.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 5])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and we can always squeeze out the zeroth index\n",
    "h_n.squeeze(dim=0).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional\n",
    "Bidirectional RNNs are a slight variant of normal RNNs, which are super useful in sequence-to-sequence models. In this repository we take individual characters to be tokens, and words (in our case, names) to be sequences. But if we wanted instead to train a machine to translate between two languages, we'd be better off having tokens be entire words, and sequences sentences. Rao's book gives the example of the sentence 'A man who hunts ducks oout on weekend.' A unidirectional RNN would read the sentence from left to right, and would interpret ducks as a noun. However, once we've read the whole sentence, we would actualy interpret ducks as a verb. It would be useful to have a model which could read sentences both ways! This is actually easily achieved, simply by working with ***bidirectional RNNs***, which are nothing but two parallel RNNs running at the same time. Let's explain how.\n",
    "\n",
    "Let $f(x,h)$ denote a single RNN cell, as before. Suppose our sequence consists of vectors $(x_1,\\ldots,x_l)$. In one direction, we run our standard RNN, which takes as an input a hidden vector $h_0$, and spits out a final $h_n$. Recall this is done ieratively as: \n",
    "- $h_1 = f(x_1,h_0)$\n",
    "- $h_2 = f(x_2,h_1)$\n",
    "- ...\n",
    "- $h_n = f(x_n,h_{n-1}$\n",
    "\n",
    "At the same time, we have another RNN, taking as input a hidden vector $k_0$ and spitting out a final $k_n$. These are defined iteratively as: \n",
    "- $k_1 = f(x_n,k_0$\n",
    "- $k_2 = f(x_{n-1},k_1)$\n",
    "- $k_3 = f(x_{n-2},k_2)$\n",
    "- ...\n",
    "- $k_n = f(x_1,k_{n-1})$\n",
    "\n",
    "The vectors $h_n$, $k_n$ are then concatenated to produce the total output of the bidirectional RNN.\n",
    "Let's implement one!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "batch_len = 4\n",
    "seq_len = 15\n",
    "input_size = 7\n",
    "hidden_size = 5\n",
    "birnnlayer = nn.GRU(input_size=input_size,\n",
    "                 hidden_size=hidden_size,\n",
    "                 num_layers=1,\n",
    "                 bias=True,\n",
    "                 batch_first=True,\n",
    "                 dropout=0,\n",
    "                 bidirectional=True)\n",
    "\n",
    "x = torch.randn(batch_len,seq_len,input_size)\n",
    "H, h = birnnlayer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 15, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `H` is the collection of all hidden vectors.\n",
    "- 4 is the batch length\n",
    "- 15 is the sequence length\n",
    "- 10 = hidden_size $*$ num_directions = 5 $*$ 2\n",
    "So the last component is stacking the hidden outputs from both directions on top of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here `h` is doing something slightly different.\n",
    "- 4 is the batch length\n",
    "- 5 is hidden size\n",
    "- 2 = num_directions\n",
    "So `h` just stacks the final output vectors from both directions, along a new dummy index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 5]) torch.Size([4, 5])\n"
     ]
    }
   ],
   "source": [
    "h_f = h[0]\n",
    "h_b = h[1]\n",
    "print(h_f.shape, h_b.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packing\n",
    "Since sequences tend to be of different lengths, there's always a need for padding. Instead of looking up in advance the length of the longest sequence, we can work batch by batch. Packed sequences are an efficient way to do that. That's it.\n",
    "\n",
    "``` Note: A sequence must be sorted by length before being packed. ```\n",
    "\n",
    "For the example below assume our vocabulary maps letters to indices alphabetically. We will pack the batch consisting of sequences 'abcd', 'efg,', 'h'.\n",
    "\n",
    "The example is taken from here: https://github.com/joosthub/PyTorchNLPBook/blob/master/chapters/chapter_8/8_PackedSequence_example.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 0.],\n",
       "        [8., 0., 0., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "abcd_padded = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
    "efg_padded = torch.tensor([5,6,7,0], dtype=torch.float32)\n",
    "h_padded = torch.tensor([8,0,0,0], dtype=torch.float32)\n",
    "\n",
    "padded_tensor = torch.stack([abcd_padded, efg_padded, h_padded])\n",
    "\n",
    "padded_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([1., 5., 8., 2., 6., 3., 7., 4.]), batch_sizes=tensor([3, 2, 2, 1]), sorted_indices=None, unsorted_indices=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths = [4,3,1]\n",
    "\n",
    "packed_tensor = pack_padded_sequence(padded_tensor, lengths, batch_first=True)\n",
    "\n",
    "packed_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 3, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4.],\n",
       "        [5., 6., 7., 0.],\n",
       "        [8., 0., 0., 0.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpacked_tensor, unpacked_lengths = pad_packed_sequence(packed_tensor, batch_first=True)\n",
    "print(lengths)\n",
    "unpacked_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
