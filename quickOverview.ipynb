{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "The point of this notebook is to quickly go through the moving parts of training the model and generating samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary class\n",
    "The point of this class is to set up a bijection between tokens (which in this case are characters) and natural numbers. The class has methods to look up tokens and ids. It also introduces some funny tokens to mark begin, end, unkown token, mask (used for padding when working with sequences of fixed length), and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary length is 31.\n",
      "To id 0 corresponds token <mask>.\n",
      "To id 1 corresponds token <begin>.\n",
      "To id 2 corresponds token <end>.\n",
      "To id 3 corresponds token <unk>.\n",
      "To id 4 corresponds token  .\n",
      "To id 5 corresponds token a.\n",
      "To id 6 corresponds token l.\n",
      "To id 7 corresponds token i.\n",
      "To id 8 corresponds token y.\n",
      "To id 9 corresponds token h.\n",
      "Empty space token corresponds to index 4.\n",
      "Token 'a' corresponds to index 5.\n"
     ]
    }
   ],
   "source": [
    "from charvocabulary import charVocabulary\n",
    "vocab = charVocabulary()\n",
    "import os\n",
    "txt_path = os.path.join('source', 'firstnames.txt')\n",
    "vocab.add_txt(txt_path)\n",
    "print(f\"Vocabulary length is {len(vocab)}.\")\n",
    "for i in range(10):\n",
    "    print(f\"To id {i} corresponds token {vocab.lookup_idx(i)}.\")\n",
    "print(f\"Empty space token corresponds to index {vocab.lookup_token(' ')}.\")\n",
    "print(f\"Token 'a' corresponds to index {vocab.lookup_token('a')}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer class\n",
    "The goal of this class is to take in a sequence of tokens, and spit out a sequence of integers.\n",
    "This class requires we already have a _vocabulary_ class.\n",
    "\n",
    "The point of the vectorizer class is the vectorize method, the workflow of which goes something like this.\n",
    "- Given a sequence, such as 'monkey', it is encoded as a list of integers (using the lookup_idx method from the vocabulary class), so we'll get something like this [1,5,13,6,9,10,5,2,0,0,0,0]. The zeros come from the padding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, we actually want to feed the network two versions of the sequence (one to be used as training input, the other for training labels).\n",
    "* x = \"begintoken\" monkey \"masktoken\"\"masktoken\"\"masktoken\"\n",
    "* y = monkey \"endtoken\"\"masktoken\"\"masktoken\"\"masktoken\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charvectorizer import charVectorizer\n",
    "vectorizer = charVectorizer(vocab=vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 24 16 14 25 13  8  0  0  0  0  0  0  0  0  0  0  0  0]\n",
      "[24 16 14 25 13  8  2  0  0  0  0  0  0  0  0  0  0  0  0]\n"
     ]
    }
   ],
   "source": [
    "x,y = vectorizer.vectorize('monkey', max_len=20)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model\n",
    "The logic behind the Embedding and RNN layers are explained in separate notebooks. You could have a look at them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from charmodel import charModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# it's useful for the model to know which id is used for padding\n",
    "maskid = vocab.mask_idx\n",
    "\n",
    "# verbose is just a silly option we added so it prints some information as it runs\n",
    "model = charModel(vocab_size=len(vocab), padding_idx=maskid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some bogus inputs\n",
    "# recall x comes from the vectorizer, although any string of natural numbers would do\n",
    "import torch\n",
    "x_in = torch.tensor(x)\n",
    "x_in = x_in.unsqueeze(dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 19])\n",
      "tensor([[ 1, 24, 16, 14, 25, 13,  8,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0]])\n"
     ]
    }
   ],
   "source": [
    "print(x_in.shape)\n",
    "print(x_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input has shape torch.Size([1, 19]).\n",
      "Output of embedding layer has shape torch.Size([1, 19, 10]).\n",
      "Output of RNN has shape torch.Size([1, 19, 9]).\n",
      "Reshaped output of RNN has shape torch.Size([19, 9]).\n",
      "Output of fc has shape torch.Size([19, 31]).\n",
      "Final output has shape torch.Size([1, 19, 31]).\n"
     ]
    }
   ],
   "source": [
    "# the verbose option just prints out the shapes of the tensors as they are processed by the model\n",
    "y = model(x_in, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding\n",
    "We feed pytorch a word, and it spits out a sequence of vectors. How do we translate this back into a word?\n",
    "\n",
    "The idea is to create an empty list called `sequence`. We first append the `<begin>` token. Then we iteratively add tokens to it, by feeding it to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of probabilities is torch.Size([1, 31]).\n",
      "The output is: bcnswuaotftmjdd.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "If you run this code and the output word looks empty, it probably generated a bunch of spaces. Simply run this cell again.\n",
    "\"\"\"\n",
    "# let's decide on the length of the word we'll generate\n",
    "sample_size = 20\n",
    "\n",
    "# we start our sequence with the begin token\n",
    "beginid = vocab.begin_idx\n",
    "# we convert it to a torch tensor, and unsqueeze a batch index\n",
    "begintensor = torch.tensor([beginid]).unsqueeze(dim=0)\n",
    "\n",
    "# we initialize our sequence\n",
    "sequence = [begintensor]\n",
    "\n",
    "# base step of our iteration\n",
    "t = 1\n",
    "x_t = sequence[1-1]\n",
    "h_t = None\n",
    "\n",
    "# once is a dummy variable to print something only once\n",
    "once = True\n",
    "\n",
    "for t in range(1,sample_size+1):\n",
    "    # input at time t is x_t\n",
    "    x_t = sequence[t-1]\n",
    "    emb_t = model.emb(x_t)\n",
    "    \n",
    "    # on the LHS h_t is the hidden output at time t, while on the LHS h_t is the old hidden vector (so h_t-1)\n",
    "    rnn_t, h_t = model.rnn(emb_t, h_t)\n",
    "    \n",
    "    # we first squeez rnn_t so that it has shape (batch, features), so it can be fed to the linear layer\n",
    "    predictions = model.fc(rnn_t.squeeze(dim=1))\n",
    "    \n",
    "    # apply a softmax to translate the outputs into probabilities\n",
    "    # softmax is applied batch-by-batch, that's why there's a dim=1\n",
    "    probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
    "    if once:\n",
    "        print(f\"The shape of probabilities is {probabilities.shape}.\")\n",
    "        once = False\n",
    "\n",
    "    # multinomial introduces an element of randomness\n",
    "    # it treats each row of probabilities as weights for a categorical distribution and then samples from it\n",
    "    # it returns the corresponding index\n",
    "    winner = torch.multinomial(probabilities, num_samples=1)\n",
    "    sequence.append(winner)\n",
    "\n",
    "temp = \"\"\n",
    "for i in range(len(sequence)):\n",
    "    idx = sequence[i].item()\n",
    "    temp += vocab.lookup_idx(idx)\n",
    "\n",
    "# what follows is a bit ad hoc, and might not play well with the unknown\n",
    "i = 0\n",
    "while i < len(temp) and temp[i] != '>':\n",
    "    i += 1\n",
    "\n",
    "decoded = \"\"\n",
    "i += 1\n",
    "while i < len(temp) and temp[i] != '<':\n",
    "    decoded += temp[i]\n",
    "    i += 1\n",
    "\n",
    "print(f\"The output is: {decoded}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "Now we know how to fully use a model. How to train it?\n",
    "\n",
    "#### Loading data\n",
    "First we need to a way to access training data. We use pytorch's dataset class and the output of the vectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 10 17  8 14  6 13 13  0  0  0]\n",
      "[10 17  8 14  6 13 13  2  0  0  0]\n",
      "Recall that the mask, begin, and end tokens correspond to indices 0, 1, and 2.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "txt_path = os.path.join('source', 'firstnames.txt')\n",
    "corpus = pd.read_csv(txt_path, header=None).dropna().reset_index()[0]\n",
    "# charDataset is a very standard pytorch dataset class\n",
    "# the getitem method returns the output of the vectorizer\n",
    "from chardataset import charDataset\n",
    "ds = charDataset(vectorizer=vectorizer, corpus=corpus)\n",
    "\n",
    "# let's pick a random sample\n",
    "from random import randint\n",
    "N = len(ds)\n",
    "x,y = ds.__getitem__(randint(0,N))\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "print(f\"Recall that the mask, begin, and end tokens correspond to indices {vocab.mask_idx}, {vocab.begin_idx}, and {vocab.end_idx}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "We use a standard cross-entropy loss. The annoying thing is we need to do some reshaping first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11, 31])\n",
      "torch.Size([11])\n",
      "The loss is 3.5431160926818848.\n"
     ]
    }
   ],
   "source": [
    "# convert data to torch and add a batch index\n",
    "x_tensor = torch.tensor(x).unsqueeze(dim=0)\n",
    "y_tensor = torch.tensor(y).unsqueeze(dim=0)\n",
    "\n",
    "# apply model\n",
    "y_pred = model(x_tensor)\n",
    "\n",
    "# reshape to apply loss\n",
    "batch_size, sequence_length, feature_dimension = y_pred.shape\n",
    "y_pred_reshaped = y_pred.view(batch_size*sequence_length, feature_dimension)\n",
    "print(y_pred_reshaped.shape)\n",
    "\n",
    "# y only contains the labels\n",
    "y_reshaped = y_tensor.view(batch_size*sequence_length)\n",
    "# the code above is equivalent to\n",
    "# y_reshaped = y_tensor.view(-1)\n",
    "print(y_reshaped.shape)\n",
    "\n",
    "# cross entropy applies a softmax first, so we don't need to\n",
    "loss = torch.nn.functional.cross_entropy(y_pred_reshaped, y_reshaped, ignore_index=maskid)\n",
    "print(f\"The loss is {loss.item()}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "We conclude this by having a look at what the training workflow looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "This is epoch number 1.\n",
      "Model has completed epoch number 1. We will now print 5 names of length 20.\n",
      "slyfan\n",
      "enaba\n",
      "sabia\n",
      "jaidlyne\n",
      "kid\n",
      "\n",
      "This is epoch number 2.\n",
      "Model has completed epoch number 2. We will now print 5 names of length 20.\n",
      "ahllia\n",
      "roda\n",
      "chain\n",
      "aysollyi\n",
      "kayya\n",
      "\n",
      "This is epoch number 3.\n",
      "Model has completed epoch number 3. We will now print 5 names of length 20.\n",
      "brya\n",
      "ragaralie\n",
      "avie\n",
      "kalinia\n",
      "onla\n",
      "\n",
      "This is epoch number 4.\n",
      "Model has completed epoch number 4. We will now print 5 names of length 20.\n",
      "callee\n",
      "hena\n",
      "alientalinel\n",
      "harlenn\n",
      "rene\n",
      "\n",
      "This is epoch number 5.\n",
      "Model has completed epoch number 5. We will now print 5 names of length 20.\n",
      "filanty\n",
      "sel\n",
      "doun\n",
      "lyna\n",
      "mar\n"
     ]
    }
   ],
   "source": [
    "# load the dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "dl = DataLoader(ds, batch_size=4, shuffle=True)\n",
    "\n",
    "# let's use a simple stochastic gradient descent as optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n",
    "\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nThis is epoch number {epoch+1}.\")\n",
    "    model.train()\n",
    "    \n",
    "    for _,data in enumerate(dl):\n",
    "        \n",
    "        # zero out gradients before we forget\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # unpack data into input and output\n",
    "        x,y = data\n",
    "        \n",
    "        # no need to convert data to tensor or unsqueeze a batch index\n",
    "        # 'tis the magic of the dataloader\n",
    "        y_pred = model(x)\n",
    "\n",
    "        batch_size, seq_len, feats = y_pred.shape\n",
    "        y_pred_loss = y_pred.view(batch_size*seq_len,feats)\n",
    "        y_loss = y.view(-1)\n",
    "        \n",
    "        # compute loss and gradients\n",
    "        loss = torch.nn.functional.cross_entropy(y_pred_loss, y_loss, ignore_index=maskid)\n",
    "        loss.backward()\n",
    "        # update\n",
    "        optimizer.step()\n",
    "\n",
    "    # instead of evaluating the model computing some accuracy score, we have it generate words\n",
    "    # we use the code from the Decoding cell above,\n",
    "    # but it would be more practical to wrap this in a function\n",
    "    \n",
    "    num_samples = 5\n",
    "    sample_size = 20\n",
    "    print(f\"Model has completed epoch number {epoch+1}. We will now print {num_samples} names of length {sample_size}.\")\n",
    "    model.eval()\n",
    "    for i in range(num_samples):\n",
    "        beginid = vocab.begin_idx\n",
    "        begintensor = torch.tensor([beginid]).unsqueeze(dim=0)\n",
    "        sequence = [begintensor]\n",
    "        t = 1\n",
    "        x_t = sequence[1-1]\n",
    "        h_t = None\n",
    "\n",
    "        for t in range(1,sample_size+1):\n",
    "            x_t = sequence[t-1]\n",
    "            emb_t = model.emb(x_t)\n",
    "            rnn_t, h_t = model.rnn(emb_t, h_t)\n",
    "            predictions = model.fc(rnn_t.squeeze(dim=1))\n",
    "            probabilities = torch.nn.functional.softmax(predictions, dim=1)\n",
    "            winner = torch.multinomial(probabilities, num_samples=1)\n",
    "            sequence.append(winner)\n",
    "\n",
    "        temp = \"\"\n",
    "        for i in range(len(sequence)):\n",
    "            idx = sequence[i].item()\n",
    "            temp += vocab.lookup_idx(idx)\n",
    "\n",
    "        i = 0\n",
    "        while i < len(temp) and temp[i] != '>':\n",
    "            i += 1\n",
    "\n",
    "        decoded = \"\"\n",
    "        i += 1\n",
    "        while i < len(temp) and temp[i] != '<':\n",
    "            decoded += temp[i]\n",
    "            i += 1\n",
    "        print(decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
